{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Web Scraping Fundamentals and Data Acquisition with Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML (Hypertext Markup Language)\n",
    "\n",
    "HTML is the standard markup language for creating Web pages. Its elements tell the browser how to display the content. \n",
    "\n",
    "HTML **tags** are element names surrounded by angle brackets:\n",
    "`<tag_name>content.....</tag_name>`\n",
    "\n",
    "For instance, HTML links are defined with the `<a>` tag. \n",
    "\n",
    "`<a href=\"https://www.google.com\">This is a link</a>`\n",
    "\n",
    "<a href=\"https://www.google.com\">This is a link</a>\n",
    "\n",
    "**Attributes** are used to provide additional information about HTML elements. The link's destination above is specified in the _href_ attribute. \n",
    "\n",
    "The `style` attribute is used to specify the styling of an element, like color, font, size etc.\n",
    "`<p style=\"color:red\">Paragraph in red..</p>`\n",
    "\n",
    "<p style=\"color:red\">Paragraph in red</p>\n",
    "\n",
    "Some attributes that are particularly important for web scraping are:\n",
    "- `class`: The HTML class attribute is used to define equal styles for elements with the same class name. All HTML elements with the same class attribute will get the same style.\n",
    "- `id`: The id attribute specifies a unique id for an HTML element.\n",
    "- `href`: Hyperlink url (important for web-crawling).\n",
    "\n",
    "An HTML element can only have one unique id that belongs to that single element, while a class name can be used by multiple elements.\n",
    "\n",
    "[source](https://www.w3schools.com/html/default.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XPath Notation\n",
    "\n",
    "XPath uses a path notation (as in URLs) for navigating through the hierarchical structure of an XML document. It uses a non-XML syntax so that it can be used in URIs and XML attribute values.\n",
    "\n",
    "Expression|Description\n",
    "-|-\n",
    "`/`|Selects from the root node\n",
    "`//`|Selects nodes in the document from the current node that match the selection no matter where they are\n",
    "`.`|Selects the current node\n",
    "`..`|Selects the parent of the current node\n",
    "`@`|Selects attributes\n",
    "`*`|Wildcard\n",
    "\n",
    "- `xpath = '/html/body/div[2]/p'`: Specific paragraph element\n",
    "- `xpath = '//p'`: All paragraph elements\n",
    "- `xpath = '//div[@id=\"uid\"]'`: Looks at all `div` elements for the case that attribute `id` equals `uid`.\n",
    "- `xpath = '//*[@id=\"div3\"]/p'`: Select the paragraph element with the id \"div3\"\n",
    "- `xpath = '//p[@id=\"p1\"]/a/@href'`: Select url from an `a` tag.\n",
    "\n",
    "`/html/body/*` selects all elements one generation below the body element without concern of the tag type, it selects all children of the body element. On the other hand, `/html/body//*` selects all elements from all future generations of the body element (that is, all descendants of the body) regardless of tag type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival. It can be used for large scale web scraping.\n",
    "\n",
    "<img width=\"500\" alt=\"Scrapy\" src=\"https://docs.scrapy.org/en/0.20/_images/scrapy_architecture.png\">\n",
    "\n",
    "[source](https://docs.scrapy.org/en/0.20/topics/architecture.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selector Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "# Create a string with the html code\n",
    "html = '''\n",
    "<html>\n",
    "<body>\n",
    "<div class=\"hello selector\">\n",
    "        <p>Hello World!</p>\n",
    "</div>\n",
    "<p>Thank you!</p> </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Create a scrapy Selector object\n",
    "sel = Selector(text = html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Selector xpath=None data='<html>\\n<body>\\n<div class=\"hello selec...'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//p' data='<p>Hello World!</p>'>,\n",
       " <Selector xpath='//p' data='<p>Thank you!</p>'>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SelectorList of all <p> elements in the HTML document\n",
    "sel.xpath(\"//p\")\n",
    "# Output is a SelectorList of Selector objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>Hello World!</p>', '<p>Thank you!</p>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data from a SelectorList\n",
    "sel.xpath(\"//p\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Hello World!</p>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first element from a SelectorList\n",
    "sel.xpath(\"//p\").extract_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Thank you!</p>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the second element from a SelectorList\n",
    "sel.xpath(\"//p\")[1].extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests Library\n",
    "\n",
    "The `requests` library is the standard for making HTTP requests in Python.\n",
    "\n",
    "Note: Although it is possible to do web scraping only with `scrapy` (without `requests`), it is good to learn `requests` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 elements in the HTML document!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Get a webpage and create a Response object\n",
    "url = 'https://api.github.com/events'\n",
    "r = requests.get(url)\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "html = r.content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector(text = html)\n",
    "\n",
    "# Print out the number of elements in the HTML document\n",
    "print(f\"There are {len( sel.xpath('//*'))} elements in the HTML document!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Objects\n",
    "\n",
    "The `Response` is similar to the `Selector` such that we can use `xpath` and `css` methods and chain them with `extract`. Both Response and Selector objects return a `SelectorList` when using the `xpath` or `css` methods. The main difference is that, `Response` can also keep tack of the **url** where the HTML is loaded from. Moreover, it lets us follow a new link with the `follow()` method. This is particularly useful when _crawling_ the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS (Cascading Style Sheets)\n",
    "\n",
    "CSS Locator notation can be used as an alternative to XPath using the `.css` method of the `Selector`. One advantage of using CSS Locator is that it often makes **attribute selection** very easy.\n",
    "\n",
    "\n",
    "- `/` replace by `>` (except first character) \n",
    "    - XPath: `/html/body/div`\n",
    "    - CSS Locator: `html > body > div`\n",
    "- `//` replaced by a blank space (except first character)\n",
    "    - XPath: `//div/span//p`\n",
    "    - CSS Locator: `div > span p`\n",
    "- `[N]` replaced by :nth-of-type(N)\n",
    "    - XPath: `//div/p[2]`\n",
    "    - CSS Locator: `div > p:nth-of-type(2)`\n",
    "- To find an element by class:\n",
    "    - XPath: `/div[class=\"courses\"]/a`\n",
    "    - CSS Locator: `div.courses > a`\n",
    "- To find an element by id:\n",
    "    - XPath: `//div[@id=\"uid\"]/span//h4`\n",
    "    - CSS Locator: `div#uid > span h4`\n",
    "\n",
    "- Attribute selection:\n",
    "    - XPath: `//div[@id=\"uid\"]/a/@href`\n",
    "    - CSS Locator: ``\n",
    "- Text selection:\n",
    "    - XPath: `//p[@id=\"p-example\"]/text()`\n",
    "    - CSS Locator: `p#p-example::text`\n",
    "- Text selection 2:\n",
    "    - XPath: `//p[@id=\"p-example\"]//text()`\n",
    "    - CSS Locator: `p#p-example ::text`\n",
    "\n",
    "The wildcard `*` can also be used in CSS Locators. For example, `'#uid > *'` selects all children (regardless of tag-type) of the unique element in the HTML document that has its `id` attribute equal to `uid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.google.com']\n",
      "['http://www.google.com']\n"
     ]
    }
   ],
   "source": [
    "# Create a string with the html code\n",
    "html = '''\n",
    "<html>\n",
    "<body>\n",
    "<div class=\"hello-selector\">\n",
    "        <p id=\"p1\">Hello World!</p>\n",
    "        <a href=\"http://www.google.com\">Google</a>\n",
    "</div>\n",
    "<div class=\"hello-selector\">\n",
    "        <p id=\"p2\">Hello <a href=\"http://www.bing.com\">Bing</a> World!</p>\n",
    "</div>\n",
    "<p>Thank you!</p> </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Create a selector object from a secret website\n",
    "sel = Selector(text = html)\n",
    "\n",
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css('div.hello-selector > a')\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css('::attr(href)')\n",
    "print(hrefs_from_css.extract())\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath('./@href')  # needs a glue (.)\n",
    "print(hrefs_from_xpath.extract())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.google.com', 'http://www.bing.com']\n",
      "['http://www.google.com', 'http://www.bing.com']\n"
     ]
    }
   ],
   "source": [
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css('div.hello-selector a')\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css('::attr(href)')\n",
    "print(hrefs_from_css.extract())\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath('./@href') # needs a glue (.)\n",
    "print(hrefs_from_xpath.extract())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting top level text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello ', ' World!']\n",
      "['Hello ', ' World!']\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p2\"]/text()'\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p2::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print(sel.xpath(xpath).extract())\n",
    "print(sel.css(css_locator).extract())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting all level text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello ', 'Bing', ' World!']\n",
      "['Hello ', 'Bing', ' World!']\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p2\"]//text()'\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p2 ::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print(sel.xpath(xpath).extract())\n",
    "print(sel.css(css_locator).extract())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiders\n",
    "\n",
    "Spiders are classes which define how a certain site (or a group of sites) will be scraped, including how to perform the crawl (i.e. follow links) and how to extract structured data from their pages (i.e. scraping items). In other words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular site (or, in some cases, a group of sites). [source](https://docs.scrapy.org/en/latest/topics/spiders.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile quotes_spider.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/tag/humor/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.xpath('span/small/text()').get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-26 23:14:32 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)\n",
      "2019-12-26 23:14:32 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.3 (default, Mar 27 2019, 16:54:48) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.7, Platform Darwin-19.2.0-x86_64-i386-64bit\n",
      "2019-12-26 23:14:32 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'json', 'FEED_URI': 'quotes.json', 'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2019-12-26 23:14:32 [scrapy.extensions.telnet] INFO: Telnet Password: f841ffd460bdbadb\n",
      "2019-12-26 23:14:32 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-12-26 23:14:32 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-12-26 23:14:32 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-12-26 23:14:32 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-12-26 23:14:32 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-12-26 23:14:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-12-26 23:14:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-12-26 23:14:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/tag/humor/> (referer: None)\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': '“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”', 'author': 'Garrison Keillor'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': '“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”', 'author': 'Jim Henson'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': \"“All you need is love. But a little chocolate now and then doesn't hurt.”\", 'author': 'Charles M. Schulz'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': \"“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\", 'author': 'Suzanne Collins'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': '“Some people never go crazy. What truly horrible lives they must lead.”', 'author': 'Charles Bukowski'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': '“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”', 'author': 'Terry Pratchett'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': '“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”', 'author': 'Dr. Seuss'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\n",
      "{'text': '“The reason I talk to myself is because I’m the only one whose answers I accept.”', 'author': 'George Carlin'}\n",
      "2019-12-26 23:14:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/tag/humor/page/2/> (referer: http://quotes.toscrape.com/tag/humor/)\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/page/2/>\n",
      "{'text': '“I am free of all prejudice. I hate everyone equally. ”', 'author': 'W.C. Fields'}\n",
      "2019-12-26 23:14:33 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/page/2/>\n",
      "{'text': \"“A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\", 'author': 'Jane Austen'}\n",
      "2019-12-26 23:14:33 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-12-26 23:14:33 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: quotes.json\n",
      "2019-12-26 23:14:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 511,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 3725,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 0.35372,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 12, 26, 22, 14, 33, 316662),\n",
      " 'item_scraped_count': 12,\n",
      " 'log_count/DEBUG': 14,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 48840704,\n",
      " 'memusage/startup': 48836608,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2019, 12, 26, 22, 14, 32, 962942)}\n",
      "2019-12-26 23:14:33 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider quotes_spider.py -o quotes.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>“The person, be it gentleman or lady, who has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steve Martin</td>\n",
       "      <td>“A day without sunshine is like, you know, nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Garrison Keillor</td>\n",
       "      <td>“Anyone who thinks sitting in church can make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jim Henson</td>\n",
       "      <td>“Beauty is in the eye of the beholder and it m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles M. Schulz</td>\n",
       "      <td>“All you need is love. But a little chocolate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>“Remember, we're madly in love, so it's all ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Charles Bukowski</td>\n",
       "      <td>“Some people never go crazy. What truly horrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Terry Pratchett</td>\n",
       "      <td>“The trouble with having an open mind, of cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>“Think left and think right and think low and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>George Carlin</td>\n",
       "      <td>“The reason I talk to myself is because I’m th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>W.C. Fields</td>\n",
       "      <td>“I am free of all prejudice. I hate everyone e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>“A lady's imagination is very rapid; it jumps ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               text\n",
       "0         Jane Austen  “The person, be it gentleman or lady, who has ...\n",
       "1        Steve Martin  “A day without sunshine is like, you know, nig...\n",
       "2    Garrison Keillor  “Anyone who thinks sitting in church can make ...\n",
       "3          Jim Henson  “Beauty is in the eye of the beholder and it m...\n",
       "4   Charles M. Schulz  “All you need is love. But a little chocolate ...\n",
       "5     Suzanne Collins  “Remember, we're madly in love, so it's all ri...\n",
       "6    Charles Bukowski  “Some people never go crazy. What truly horrib...\n",
       "7     Terry Pratchett  “The trouble with having an open mind, of cour...\n",
       "8           Dr. Seuss  “Think left and think right and think low and ...\n",
       "9       George Carlin  “The reason I talk to myself is because I’m th...\n",
       "10        W.C. Fields  “I am free of all prejudice. I hate everyone e...\n",
       "11        Jane Austen  “A lady's imagination is very rapid; it jumps ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_json(\"quotes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "url_short = 'https://www.asdf.com'\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "    name = 'dcdescr'\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "    # First parse method\n",
    "    def parse( self, response ):\n",
    "        links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "        # Follow each of the extracted links\n",
    "        for link in links:\n",
    "            yield response.follow(url = link, callback = self.parse_descr)\n",
    "      \n",
    "    # Second parsing method\n",
    "    def parse_descr( self, response ):\n",
    "        # Extract course description\n",
    "        course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "        # For now, just yield the course description\n",
    "        yield course_descr\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "    # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url = url,\n",
    "                                callback = self.parse_pages)\n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        # Create a SelectorList of the course titles text\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        # Extract the text and strip it clean\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        # Create a SelectorList of course descriptions text\n",
    "        crs_descr = response.css('p.course__description::text')\n",
    "        # Extract the text and strip it clean\n",
    "        crs_descr_ext = crs_descr.extract_first().strip()\n",
    "        # Fill in the dictionary\n",
    "        dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
